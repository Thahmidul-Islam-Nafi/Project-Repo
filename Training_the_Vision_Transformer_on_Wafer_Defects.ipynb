{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my test of Training the Vision Transformer on a Custom Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szWwJmqPHZ-r"
      },
      "source": [
        "## Training the Vision Transformer on a Wafer Dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_7rNZfIf301"
      },
      "source": [
        "Let's start by installing the relevant libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acZj39ATY-zW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INXObB8-jK_-"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Mixed\\ Type Wafer/\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCTZFkFw6i6"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz-kwMj1ccFP"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZMQIvJDicO9"
      },
      "source": [
        "## Downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DSSAj4Os-Od"
      },
      "source": [
        "Next, convert the folder structure dataset into a PyTorch dataset format using PyTorch's ImageFolder dataset structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUZQQTs4kDz3"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgn-a3NiSjqR"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import ToTensor,Compose,Pad\n",
        "\n",
        "# train_ds = torchvision.datasets.ImageFolder('/content/train/', transform=ToTensor())\n",
        "# valid_ds = torchvision.datasets.ImageFolder('/content/valid/', transform=ToTensor())\n",
        "# test_ds = torchvision.datasets.ImageFolder('/content/test/', transform=ToTensor())\n",
        "train_ds = torchvision.datasets.ImageFolder('Feature_Images(train_test_valid)/train', transform=Compose([Pad(86),ToTensor()]))\n",
        "valid_ds = torchvision.datasets.ImageFolder('Feature_Images(train_test_valid)/valid', transform=Compose([Pad(86),ToTensor()]))\n",
        "test_ds =torchvision.datasets.ImageFolder('Feature_Images(train_test_valid)/test', transform=Compose([Pad(86),ToTensor()]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyHJjDYLfoFy"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "Here we define the model.\n",
        "\n",
        "The model itself uses a linear layer on top of a pre-trained `ViTModel`. We place a linear layer on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. We also add dropout for regularization.\n",
        "\n",
        "**Note:** The Vision Transformer pretrained model can be used as a regular PyTorch layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGDrb1Q4ToLN"
      },
      "source": [
        "from transformers import ViTModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTForImageClassification(nn.Module):\n",
        "    def __init__(self, num_labels=38):\n",
        "        super(ViTForImageClassification, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def forward(self, pixel_values, labels):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          loss_fct = nn.CrossEntropyLoss()\n",
        "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        if loss is not None:\n",
        "          return logits, loss.item()\n",
        "        else:\n",
        "          return logits, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BePiLK-LtXuG"
      },
      "source": [
        "## Define the Model Parameters\n",
        "\n",
        "To train this model, we will train in 3 epochs, with a batch size of 10 and a learning rate of 2e-5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntGvS0_wUAxc"
      },
      "source": [
        "EPOCHS = 6\n",
        "BATCH_SIZE = 10\n",
        "LEARNING_RATE = 2e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At9H-QOStt8_"
      },
      "source": [
        "We will use the pretrained Vision Transformer feature extractor, an Adam Optimizer, and a Cross Entropy Loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIyJr8EDtvlR"
      },
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "# Define Model\n",
        "model = ViTForImageClassification(len(train_ds.classes))    \n",
        "# Feature Extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# Adam Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# Cross Entropy Loss\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# Use GPU if available  \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "if torch.cuda.is_available():\n",
        "    model.cuda() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZ-BP1yu1Us"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGwi34ZWhZuM"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2pKBmMvkAMv"
      },
      "source": [
        "# from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "\n",
        "# tensorboard_callback = TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9eAF2wofTJb"
      },
      "source": [
        "# torch.Tensor.cpu().tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ql2T5PDUI1D",
        "outputId": "2f5e5a3b-1e2b-42ef-b4ff-d4bfeed3d3ca"
      },
      "source": [
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from transformers.integrations import TensorBoardCallback\n",
        "\n",
        "\n",
        "print(\"Number of train samples: \", len(train_ds))\n",
        "print(\"Number of test samples: \", len(test_ds))\n",
        "print(\"Detected Classes are: \", train_ds.class_to_idx) \n",
        "\n",
        "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
        "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "'''\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img.reshape(224,224,3), cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")\n",
        "'''\n",
        "\n",
        "net_accuracy = []\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):    \n",
        "  epoch_accuracy=[]    \n",
        "  for step, (x, y) in enumerate(train_loader):\n",
        "    # Change input array into list with each batch being one element\n",
        "    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
        "    # Remove unecessary dimension\n",
        "    for index, array in enumerate(x):\n",
        "      x[index] = np.squeeze(array)\n",
        "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "    # Send to GPU if available\n",
        "    x, y  = x.to(device), y.to(device)\n",
        "    b_x = Variable(x)   # batch x (image)\n",
        "    b_y = Variable(y)   # batch y (target)\n",
        "    # Feed through model\n",
        "    output, loss = model(b_x, None)\n",
        "\n",
        "    # callbacks\n",
        "    TensorBoardCallback()\n",
        "\n",
        "    # Calculate loss\n",
        "    if loss is None: \n",
        "      loss = loss_func(output, b_y)   \n",
        "      optimizer.zero_grad()           \n",
        "      loss.backward()                 \n",
        "      optimizer.step()\n",
        "\n",
        "    \n",
        "    if step % 50 == 0:\n",
        "      # Get the next batch for testing purposes\n",
        "      test = next(iter(test_loader))\n",
        "      test_x = test[0]\n",
        "      # Reshape and get feature matrices as needed\n",
        "      test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
        "      for index, array in enumerate(test_x):\n",
        "        test_x[index] = np.squeeze(array)\n",
        "      test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
        "      # Send to appropirate computing device\n",
        "      test_x = test_x.to(device)\n",
        "      test_y = test[1].to(device)\n",
        "      # Get output (+ respective class) and compare to target\n",
        "      test_output, loss = model(test_x, test_y)\n",
        "      test_output = test_output.argmax(1)\n",
        "      # Calculate Accuracy\n",
        "      accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
        "      epoch_accuracy.append(accuracy)\n",
        "      net_accuracy.append(accuracy)\n",
        "      print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)\n",
        "    \n",
        "  print(f'After Epoch : {epoch}, Epoch Accuracy: {sum(epoch_accuracy) / len(epoch_accuracy):.2f}')\n",
        "  \n",
        "print(f'After Epoch : {epoch}, Total Accuracy: {sum(net_accuracy) / len(net_accuracy):.2f}')\n",
        "\n",
        "\n",
        "\n",
        "    # checkpoint_callback = ModelCheckpoint(\n",
        "    # monitor=\"val_loss\",\n",
        "    # dirpath=\"my/path/\",\n",
        "    # filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\",\n",
        "    # save_top_k=3,\n",
        "    # mode=\"min\",\n",
        "    # )\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samples:  26610\n",
            "Number of test samples:  5700\n",
            "Detected Classes are:  {'C1': 0, 'C10': 1, 'C11': 2, 'C12': 3, 'C13': 4, 'C14': 5, 'C15': 6, 'C16': 7, 'C17': 8, 'C18': 9, 'C19': 10, 'C2': 11, 'C20': 12, 'C21': 13, 'C22': 14, 'C23': 15, 'C24': 16, 'C25': 17, 'C26': 18, 'C27': 19, 'C28': 20, 'C29': 21, 'C3': 22, 'C30': 23, 'C31': 24, 'C32': 25, 'C33': 26, 'C34': 27, 'C35': 28, 'C36': 29, 'C37': 30, 'C38': 31, 'C4': 32, 'C5': 33, 'C6': 34, 'C7': 35, 'C8': 36, 'C9': 37}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | train loss: 3.6461 | test accuracy: 0.00\n",
            "Epoch:  0 | train loss: 3.5586 | test accuracy: 0.10\n",
            "Epoch:  0 | train loss: 3.2582 | test accuracy: 0.40\n",
            "Epoch:  0 | train loss: 3.2217 | test accuracy: 0.30\n",
            "Epoch:  0 | train loss: 2.7622 | test accuracy: 0.40\n",
            "Epoch:  0 | train loss: 2.8103 | test accuracy: 0.40\n",
            "Epoch:  0 | train loss: 2.3865 | test accuracy: 0.60\n",
            "Epoch:  0 | train loss: 2.4244 | test accuracy: 0.50\n",
            "Epoch:  0 | train loss: 2.0738 | test accuracy: 0.70\n",
            "Epoch:  0 | train loss: 2.1545 | test accuracy: 0.60\n",
            "Epoch:  0 | train loss: 1.9109 | test accuracy: 0.60\n",
            "Epoch:  0 | train loss: 2.0549 | test accuracy: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VM_L3SUtvOk"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWXvWiB-srBC"
      },
      "source": [
        "## Evaluate on a Test Image\n",
        "\n",
        "Finally, let's evaluate the model on a test image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d66GqzGo63VY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLv_xdYssuGO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "EVAL_BATCH = 1\n",
        "eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4) \n",
        "# Disable grad\n",
        "with torch.no_grad():\n",
        "    \n",
        "  inputs, target = next(iter(eval_loader))\n",
        "  # Reshape and get feature matrices as needed\n",
        "  print(inputs.shape)\n",
        "  inputs = inputs[0].permute(1, 2, 0)\n",
        "  # Save original Input\n",
        "  originalInput = inputs\n",
        "  for index, array in enumerate(inputs):\n",
        "    inputs[index] = np.squeeze(array)\n",
        "  inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
        "\n",
        "  # Send to appropriate computing device\n",
        "  inputs = inputs.to(device)\n",
        "  target = target.to(device)\n",
        " \n",
        "  # Generate prediction\n",
        "  prediction, loss = model(inputs, target)\n",
        "    \n",
        "  # Predicted class value using argmax\n",
        "  predicted_class = np.argmax(prediction.cpu())\n",
        "  value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
        "  value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
        "        \n",
        "  # Show result\n",
        "  plt.imshow(originalInput)\n",
        "  plt.xlim(224,0)\n",
        "  plt.ylim(224,0)\n",
        "  plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGVIM5jX2Z5b"
      },
      "source": [
        "## Save the Entire Model\n",
        "\n",
        "We can save the entire model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4S-BcSI2v88"
      },
      "source": [
        "torch.save(model, './model_6_epoch.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeJ99BfV9QRo"
      },
      "source": [
        "## Export Trained Model\n",
        "\n",
        "Now that you have trained your custom vision transformer, you can export the trained model you have made here for inference on your device elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFmfiH0Z3QJb"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# %cp /content/model.pt /content/gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORZKpeH1REHP"
      },
      "source": [
        "## Use your Exported Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS706FbORDiO"
      },
      "source": [
        "MODEL_PATH = './model_6_epoch.pt'\n",
        "# model = torch.load(MODEL_PATH, map_location='cpu')\n",
        "model = torch.load(MODEL_PATH)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oio3ZvVzDwr4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "count = 0 \n",
        "EVAL_BATCH = 1\n",
        "eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4) \n",
        "\n",
        "# Disable grad\n",
        "with torch.no_grad():\n",
        "  for i in range(len(eval_loader)):\n",
        "    inputs, target = next(iter(eval_loader))\n",
        "    # Reshape and get feature matrices as needed\n",
        "    #print(inputs.shape)\n",
        "    inputs = inputs[0].permute(1, 2, 0)\n",
        "    # Save original Input\n",
        "    originalInput = inputs\n",
        "    for index, array in enumerate(inputs):\n",
        "        inputs[index] = np.squeeze(array)\n",
        "    inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
        "\n",
        "    # Send to appropriate computing device\n",
        "    inputs = inputs.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    # Generate prediction\n",
        "    prediction, loss = model(inputs, target)\n",
        "\n",
        "    # Predicted class value using argmax\n",
        "    predicted_class = np.argmax(prediction.cpu())\n",
        "    value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
        "    value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
        "\n",
        "    if (value_predicted == value_target):\n",
        "        count+=1\n",
        "    print(count, i)\n",
        "\n",
        "print(count/len(eval_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GykZTAK5Bee"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "count = 0 \n",
        "EVAL_BATCH = 1\n",
        "eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4)\n",
        "print(len(eval_loader))\n",
        "print(len(valid_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdoAtMjD5eNA"
      },
      "source": [
        "5704-5704*.9530154277"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u67sRMAR6V7r"
      },
      "source": [
        "100/38"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}